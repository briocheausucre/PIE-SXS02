{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github.com/briocheausucre/PIE-SXS02/main/notebook_script.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import os\n",
    "\n",
    "class LlamaChatbot:\n",
    "    def __init__(self, model_name, access_token, cache_dir=\"./llama_local\"):\n",
    "        self.model_name = model_name\n",
    "        self.access_token = access_token\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        # Télécharger et stocker le tokenizer en local\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, \n",
    "            token=access_token,\n",
    "            cache_dir=cache_dir\n",
    "        )\n",
    "        \n",
    "        # Télécharger et stocker le modèle en local\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            device_map=\"auto\",\n",
    "            cache_dir=cache_dir,\n",
    "            use_auth_token=access_token\n",
    "        )\n",
    "\n",
    "        # Créer le pipeline en utilisant le modèle et le tokenizer téléchargés\n",
    "        self.meta_pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            torch_dtype=\"auto\",\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "    def get_completion(self, user_input):\n",
    "        system_prompt = \"\"\"\n",
    "        You are a conservative guy.\n",
    "        Help as much as you can.\n",
    "        \"\"\"\n",
    "        prompt = f\"#### System: {system_prompt}\\n#### User: {user_input}\\n\\n#### Response from llama:\"\n",
    "        try:\n",
    "            llama_response = self.meta_pipeline(\n",
    "                prompt,\n",
    "                max_length=500,\n",
    "                do_sample=True,\n",
    "                top_k=10,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                truncation=True  # Ajouté pour éviter l'avertissement\n",
    "            )\n",
    "            return llama_response[0]['generated_text']\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la génération de texte : {e}\")\n",
    "            return \"Désolé, je n'ai pas pu traiter votre demande.\"\n",
    "\n",
    "    def chat_loop(self):\n",
    "        print(\"Entrez 'exit' pour quitter.\")\n",
    "        while True:\n",
    "            try:\n",
    "                prompt = input(\"\\nEntrez votre prompt : \")\n",
    "                if prompt.lower() == 'exit':\n",
    "                    print(\"Chat terminé. Au revoir !\")\n",
    "                    break\n",
    "                response = self.get_completion(prompt)\n",
    "                print(\"\\nLlama Response:\\n\", response)\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nChat interrompu par l'utilisateur.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Une erreur est survenue : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Remplace ces valeurs par ton propre modèle et token d'accès\n",
    "    access_token = \"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\"\n",
    "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "    \n",
    "    # Initialiser le chatbot avec cache en local\n",
    "    chatbot = LlamaChatbot(model_name, access_token)\n",
    "    \n",
    "    # Démarrer la boucle de chat\n",
    "    chatbot.chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remplace ces valeurs par ton propre modèle et token d'accès\n",
    "access_token = \"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\"\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Initialiser le chatbot avec cache en local\n",
    "chatbot = LlamaChatbot(model_name, access_token)\n",
    "\n",
    "# Démarrer la boucle de chat\n",
    "chatbot.chat_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
