{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/briocheausucre/PIE-SXS02/blob/viken/notebook_script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5d1qAdC7JR2"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/briocheausucre/PIE-SXS02/blob/main/notebook_script.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O89bLXUH7JR3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import os\n",
        "\n",
        "class LlamaChatbot:\n",
        "    def __init__(self, model_name, access_token, cache_dir=\"./llama_local\"):\n",
        "        self.model_name = model_name\n",
        "        self.access_token = access_token\n",
        "        self.cache_dir = cache_dir\n",
        "\n",
        "        # Télécharger et stocker le tokenizer en local\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_name,\n",
        "            token=access_token,\n",
        "            cache_dir=cache_dir\n",
        "        )\n",
        "\n",
        "        # Télécharger et stocker le modèle en local\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=\"auto\",\n",
        "            trust_remote_code=True,\n",
        "            device_map=\"auto\",\n",
        "            cache_dir=cache_dir,\n",
        "            use_auth_token=access_token\n",
        "        )\n",
        "\n",
        "        # Créer le pipeline en utilisant le modèle et le tokenizer téléchargés\n",
        "        self.meta_pipeline = pipeline(\n",
        "            \"text-generation\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            torch_dtype=\"auto\",\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "    def get_completion(self, user_input):\n",
        "        system_prompt = \"\"\"\n",
        "        You are a conservative guy.\n",
        "        Help as much as you can.\n",
        "        \"\"\"\n",
        "        prompt = f\"#### System: {system_prompt}\\n#### User: {user_input}\\n\\n#### Response from llama:\"\n",
        "        try:\n",
        "            llama_response = self.meta_pipeline(\n",
        "                prompt,\n",
        "                max_length=10000,\n",
        "                do_sample=True,\n",
        "                top_k=10,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                pad_token_id=self.tokenizer.eos_token_id,\n",
        "                truncation=True  # Ajouté pour éviter l'avertissement\n",
        "            )\n",
        "            return llama_response[0]['generated_text']\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors de la génération de texte : {e}\")\n",
        "            return \"Désolé, je n'ai pas pu traiter votre demande.\"\n",
        "\n",
        "    def add_guardrail_to_prompt(self, user_prompt):\n",
        "        \"\"\"\n",
        "        Prend le prompt de l'utilisateur et ajoute un message introductif\n",
        "        pour encadrer le comportement de l'IA.\n",
        "\n",
        "        Args:\n",
        "            user_prompt (str): Le prompt fourni par l'utilisateur.\n",
        "\n",
        "        Returns:\n",
        "            str: Le prompt modifié avec le message introductif.\n",
        "        \"\"\"\n",
        "        guardrail_message = (\n",
        "            \"You are an AI assistant designed to provide accurate and respectful answers. \"\n",
        "            \"Never provide harmful or offensive responses. \\n\"\n",
        "        )\n",
        "        return f\"{guardrail_message}{user_prompt}\"\n",
        "\n",
        "    def check_response_for_problems(self, llm_response):\n",
        "        verifier_prompt = f\"\"\"\n",
        "        Analyze the following text and determine if it contains harmful, or offensive content:\n",
        "        \\\"{llm_response}\\\"\n",
        "        Respond with \"Safe\" if the text is acceptable, otherwise respond with \"Problematic\".\n",
        "        \"\"\"\n",
        "        try:\n",
        "            verification_result = self.meta_pipeline(\n",
        "                verifier_prompt,\n",
        "                max_length=10000,\n",
        "                do_sample=False,\n",
        "                top_k=1,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "            return verification_result[0]['generated_text'].strip()\n",
        "        except Exception as e:\n",
        "            print(f\"Erreur lors de la vérification : {e}\")\n",
        "            return \"Verification failed.\"\n",
        "\n",
        "\n",
        "    def chat_loop(self):\n",
        "        print(\"Entrez 'exit' pour quitter.\")\n",
        "        while True:\n",
        "            try:\n",
        "                prompt = input(\"\\nEntrez votre prompt : \")\n",
        "                prompt = self.add_guardrail_to_prompt(prompt)\n",
        "                #print(prompt)\n",
        "                if prompt.lower() == 'exit':\n",
        "                    print(\"Chat terminé. Au revoir !\")\n",
        "                    break\n",
        "\n",
        "                response = self.get_completion(prompt)\n",
        "                safety_verdict = self.check_response_for_problems(response)\n",
        "\n",
        "                print(\"\\nLlama Response:\\n\", response)\n",
        "                if \"Problematic\" in safety_verdict:\n",
        "                    print(\"\\nSafety Check: Problematic content detected. The response may not be suitable.\")\n",
        "                else:\n",
        "                    print(\"\\nSafety Check: The response is safe.\")\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\nChat interrompu par l'utilisateur.\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"Une erreur est survenue : {e}\")\n",
        "\n",
        "'''\n",
        "if __name__ == \"__main__\":\n",
        "    # Remplace ces valeurs par ton propre modèle et token d'accès\n",
        "    access_token = \"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\"\n",
        "    model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "    # Initialiser le chatbot avec cache en local\n",
        "    chatbot = LlamaChatbot(model_name, access_token)\n",
        "\n",
        "    # Démarrer la boucle de chat\n",
        "    chatbot.chat_loop()'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POUR LANCER LE CHATBOT (DISCUSSION)"
      ],
      "metadata": {
        "id": "vBhvLkDSEgaP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI5rAFoJ7JR4"
      },
      "outputs": [],
      "source": [
        "#### POUR LANCER LE CHATBOT (DISCUSSION) ####\n",
        "\n",
        "access_token = \"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\"\n",
        "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "chatbot = LlamaChatbot(model_name, access_token)\n",
        "\n",
        "chatbot.chat_loop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}